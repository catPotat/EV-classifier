{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-a85047ca-7ff1-498d-9a75-692513dc244f","output_cleared":false,"source_hash":"e6f8aaaa","execution_millis":2,"execution_start":1604212437611},"source":"import tensorflow as tf\nfrom tensorflow import keras\n\n# tf.python.client.device_lib.list_local_devices()\n# print(\"# GPUs Available: \",\n#     len(tf.config.experimental.list_physical_devices())\n# )\n","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-f8e8edad-102d-4e5f-afdb-c34932a1267f","output_cleared":false,"source_hash":"366d5474","execution_millis":12,"execution_start":1604164877727},"source":"# https://www.tensorflow.org/tutorials/images/classification\n\nimport time\nimport numpy as np\nfrom tensorflow.keras.callbacks import TensorBoard\n\n\nDATAPATH = \"D:/User/long/WORKSPACE_ALT/__coding__/ML/Keras/Dataset/sap1231-251020/Labeled/SoloEevee\"\nVAL_SPLIT = 0.15\nIMG_HEIGHT = 160\nIMG_WIDTH = 160\nBATCH_SIZE = 32\nEPOCHS = 10\nRANDOM_SEED = 6110\n\ntimestamp = int(time.time())\ntensorboard = TensorBoard(log_dir=f'tensorboard/eevee/{timestamp}')\n","execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'D:/User/long/WORKSPACE_ALT/__coding__/ML/Keras/Dataset/sap1231-251020/Labeled/SoloEevee'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6118f16d7ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m val_ds = image_dataset_from_directory(\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m       follow_links=follow_links)\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \"\"\"\n\u001b[1;32m     64\u001b[0m   \u001b[0minferred_class_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0minferred_class_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/User/long/WORKSPACE_ALT/__coding__/ML/Keras/Dataset/sap1231-251020/Labeled/SoloEevee'"]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-ebbb90d1-abd1-449f-9f13-d033ccd8cb39"},"source":"from tensorflow.keras.preprocessing import image_dataset_from_directory\n\n\ntrain_ds = image_dataset_from_directory(\n    DATAPATH,\n    # label_mode='categorical',\n    validation_split=VAL_SPLIT,\n    subset=\"training\",\n    seed=RANDOM_SEED,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    interpolation=\"bilinear\",\n)\nval_ds = image_dataset_from_directory(\n    DATAPATH,\n    validation_split=VAL_SPLIT,\n    subset=\"validation\",\n    seed=RANDOM_SEED,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n)\nprint(train_ds.class_names)\n\n\n# AUTOTUNE = tf.data.experimental.AUTOTUNE\n# train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-d1410851-6887-4122-b812-b421c237a7f9"},"source":"from tensorflow.keras import layers\n\n\ndata_augmentation = keras.Sequential([\n    layers.experimental.preprocessing.RandomFlip(\n        \"horizontal\",\n        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n    ),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomContrast(0.1),\n    layers.experimental.preprocessing.RandomTranslation(0.1, 0.1),\n])\n\n# https://keras.io/api/applications/ \"Fine-tune InceptionV3 on a new set of classes\"\n# https://keras.io/api/applications/mobilenet/\nmodel = keras.Sequential([\n    # data_augmentation,\n    layers.experimental.preprocessing.Rescaling(\n        scale=1./127.5, offset=-1, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n    keras.applications.mobilenet_v2.MobileNetV2(\n        alpha=0.35, # width multiplier\n        # depth_multiplier=1,\n        include_top=False,\n        pooling=\"avg\",\n        classes=9,\n    ),\n    # layers.GlobalAveragePooling2D(),\n    layers.Dropout(0.1),\n    layers.Dense(9,\n        # activation='softmax'\n    )\n])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-d7658df9-10ea-4a21-9448-7277b4afe5d8"},"source":"# https://keras.io/api/models/model_training_apis/\n\nmodel.compile(\n    optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0),\n    # loss='categorical_crossentropy',\n    # optimizer=keras.optimizers.Adam(lr=0.002),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-5b49c486-d2ae-4249-a79b-c178e532bec3","output_cleared":false,"source_hash":"6b8cdf78"},"source":"model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    verbose=1,\n    callbacks=[tensorboard],\n)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-9759df19-20a0-4696-ab9a-97748d65e1b4"},"source":"import tensorflowjs as tfjs\n\nmodel.save(f\"EV-classify-{timestamp}.keras\")\ntfjs.converters.save_keras_model(model, \"D:/User/long/WORKSPACE_ALT/__coding__/ML/Keras\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-5a78a61b-60e7-406f-b3a8-b9a22a1221dc","output_cleared":false,"source_hash":"54c38902","execution_millis":634,"execution_start":1604232229377},"source":"!","execution_count":24,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"823a8fe9-f880-4cc0-97b7-24a6f65c7736","deepnote_execution_queue":[]}}